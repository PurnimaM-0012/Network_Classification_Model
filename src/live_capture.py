# -*- coding: utf-8 -*-
"""Untitled1 (1).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/10oRZZD-Rjg3G7Q2juOPUXhVDASiQvnIa
"""

import nest_asyncio
import pyshark
import pandas as pd
import numpy as np
from collections import defaultdict


expected_columns = [
    'BYTES', 'BYTES_REV', 'PACKETS', 'PACKETS_REV', 'REV_MORE',
    'PKT_LENGTHS_MEAN', 'PKT_LENGTHS_MAX', 'PKT_LENGTHS_STD',
    'PKT_LENGTHS_25', 'PKT_LENGTHS_50', 'PKT_LENGTHS_75',
    'INTERVALS_MEAN', 'INTERVALS_MAX', 'INTERVALS_STD',
    'INTERVALS_25', 'INTERVALS_50', 'INTERVALS_75',
    'BRST_COUNT',
    'BRST_BYTES_MEAN', 'BRST_BYTES_MAX', 'BRST_BYTES_STD',
    'BRST_BYTES_25', 'BRST_BYTES_50', 'BRST_BYTES_75',
    'BRST_PACKETS_MEAN', 'BRST_PACKETS_MAX', 'BRST_PACKETS_STD',
    'BRST_PACKETS_25', 'BRST_PACKETS_50', 'BRST_PACKETS_75',
    'BRST_DURATION_MEAN', 'BRST_DURATION_MAX', 'BRST_DURATION_STD',
    'BRST_DURATION_25', 'BRST_DURATION_50', 'BRST_DURATION_75',
    'BRST_INTERVALS_MEAN', 'BRST_INTERVALS_MAX', 'BRST_INTERVALS_STD',
    'BRST_INTERVALS_25', 'BRST_INTERVALS_50', 'BRST_INTERVALS_75'
]

INTERFACE = "Wi-Fi"

capture = pyshark.LiveCapture(interface=INTERFACE)

flows = defaultdict(list)

print("Capturing packets... Press stop/interrupt to end.")

try:
    for packet in capture.sniff_continuously(packet_count=200):  # limit packets for demo
        try:
            ip = packet.ip
            transport = getattr(packet, 'tcp', getattr(packet, 'udp', None))
            if transport is None:
                continue

            flow_key = (ip.src, ip.dst, transport.srcport, transport.dstport, packet.highest_layer)

            ts = float(packet.sniff_timestamp)
            size = int(packet.length)

            flows[flow_key].append((ts, size))
        except AttributeError:
            # Ignore packets without IP/TCP/UDP layers
            continue
except KeyboardInterrupt:
    print("Stopped capturing.")

def extract_features(flow_packets):
    if len(flow_packets) == 0:
        return {col: 0 for col in expected_columns}

    times = np.array([p[0] for p in flow_packets])
    sizes = np.array([p[1] for p in flow_packets])

    intervals = np.diff(times) if len(times) > 1 else [0]

    features = {}

    # Basic
    features['BYTES'] = sizes.sum()
    features['BYTES_REV'] = 0  # Simplified, no reverse direction tracked yet
    features['PACKETS'] = len(sizes)
    features['PACKETS_REV'] = 0
    features['REV_MORE'] = 1 if features['BYTES_REV'] > features['BYTES'] else 0

    # Packet length stats
    features['PKT_LENGTHS_MEAN'] = sizes.mean()
    features['PKT_LENGTHS_MAX'] = sizes.max()
    features['PKT_LENGTHS_STD'] = sizes.std()
    for q, name in zip([25, 50, 75], ['25', '50', '75']):
        features[f'PKT_LENGTHS_{name}'] = np.percentile(sizes, q)

    # Intervals
    features['INTERVALS_MEAN'] = np.mean(intervals)
    features['INTERVALS_MAX'] = np.max(intervals)
    features['INTERVALS_STD'] = np.std(intervals)
    for q, name in zip([25, 50, 75], ['25', '50', '75']):
        features[f'INTERVALS_{name}'] = np.percentile(intervals, q)

    # Burst stats (simplified: each packet = one burst)
    burst_sizes = sizes
    burst_packets = np.ones_like(sizes)
    burst_durations = np.ones_like(sizes)
    burst_intervals = intervals if len(intervals) > 0 else [0]

    features['BRST_COUNT'] = len(burst_sizes)

    for arr, prefix in [
        (burst_sizes, "BRST_BYTES"),
        (burst_packets, "BRST_PACKETS"),
        (burst_durations, "BRST_DURATION"),
        (burst_intervals, "BRST_INTERVALS")
    ]:
        features[f'{prefix}_MEAN'] = np.mean(arr)
        features[f'{prefix}_MAX'] = np.max(arr)
        features[f'{prefix}_STD'] = np.std(arr)
        for q, name in zip([25, 50, 75], ['25', '50', '75']):
            features[f'{prefix}_{name}'] = np.percentile(arr, q)

    return features

# ================
# 4. Build final dataframe
# ================
records = []
for flow_key, pkts in flows.items():
    record = extract_features(pkts)
    records.append(record)

live_df = pd.DataFrame(records)

# Align with Kaggle dataset columns
live_df = live_df.reindex(columns=expected_columns, fill_value=0)

print("✅ Live dataframe ready!")
print(live_df.head())

live_df.shape

import joblib

# Load the trained model and scaler
model = joblib.load("traffic_classifier.pkl")
scaler = joblib.load("scaler.pkl")

print(" Model and Scaler loaded successfully")

# ===============================
# 1. Define Kaggle training columns (as you pasted)
# ===============================
training_columns = [
    'BYTES','BYTES_REV','PACKETS','PACKETS_REV','TYPE',
    'DBI_BRST_BYTES','DBI_BRST_PACKETS','PKT_LENGTHS','PPI_PKT_DIRECTIONS',
    'BRST_COUNT','REV_MORE','PKT_TIMES','DBI_BRST_TIME_START','DBI_BRST_TIME_STOP',
    'DBI_BRST_DURATION','DBI_BRST_INTERVALS','PKT_LENGTHS_MEAN','PKT_LENGTHS_MAX',
    'PKT_LENGTHS_STD','PKT_LENGTHS_25','PKT_LENGTHS_50','PKT_LENGTHS_75',
    'TIME_INTERVALS','INTERVALS_MEAN','INTERVALS_MAX','INTERVALS_STD',
    'INTERVALS_25','INTERVALS_50','INTERVALS_75',
    'BRST_BYTES_MEAN','BRST_BYTES_MAX','BRST_BYTES_STD',
    'BRST_BYTES_25','BRST_BYTES_50','BRST_BYTES_75',
    'BRST_PACKETS_MEAN','BRST_PACKETS_MAX','BRST_PACKETS_STD',
    'BRST_PACKETS_25','BRST_PACKETS_50','BRST_PACKETS_75',
    'BRST_INTERVALS_MEAN','BRST_INTERVALS_MAX','BRST_INTERVALS_STD',
    'BRST_INTERVALS_25','BRST_INTERVALS_50','BRST_INTERVALS_75',
    'BRST_DURATION_MEAN','BRST_DURATION_MAX','BRST_DURATION_STD',
    'BRST_DURATION_25','BRST_DURATION_50','BRST_DURATION_75',
    'PCA_DBI_BRST_BYTES_0','PCA_DBI_BRST_BYTES_1',
    'PCA_DBI_BRST_PACKETS_0','PCA_DBI_BRST_PACKETS_1',
    'PCA_DBI_BRST_TIME_START_0','PCA_DBI_BRST_TIME_START_1',
    'PCA_DBI_BRST_TIME_STOP_0','PCA_DBI_BRST_TIME_STOP_1',
    'PCA_PKT_LENGTHS_0','PCA_PKT_LENGTHS_1',
    'PCA_BRST_INTERVALS_0','PCA_BRST_INTERVALS_1',
    'PCA_BRST_DURATION_0','PCA_BRST_DURATION_1',
    'BRST_BYTES_0','BRST_BYTES_1','BRST_BYTES_2','BRST_BYTES_3','BRST_BYTES_4',
    'BRST_BYTES_5','BRST_BYTES_6','BRST_BYTES_7','BRST_BYTES_8','BRST_BYTES_9',
    'BRST_PACKETS_0','BRST_PACKETS_1','BRST_PACKETS_2','BRST_PACKETS_3',
    'BRST_PACKETS_4','BRST_PACKETS_5','BRST_PACKETS_6','BRST_PACKETS_7',
    'BRST_PACKETS_8','BRST_PACKETS_9',
    'DBI_BRST_BYTES_mean','DBI_BRST_BYTES_std','DBI_BRST_BYTES_min',
    'DBI_BRST_BYTES_max','DBI_BRST_BYTES_len'
]

# ===============================
# 2. Map live features -> training features
# ===============================
rename_map = {
    # Your extractor produces BRST_* names, Kaggle has DBI_BRST_* in some places
    'BRST_BYTES_MEAN': 'DBI_BRST_BYTES_mean',
    'BRST_BYTES_STD': 'DBI_BRST_BYTES_std',
    'BRST_BYTES_MAX': 'DBI_BRST_BYTES_max',
    # Note: you don’t generate min/len → we’ll fill them as 0
    # Same pattern applies for other feature groups if needed
}

# Apply renaming
aligned_df = live_df.rename(columns=rename_map)

# ===============================
# 3. Add missing training columns with default = 0
# ===============================
for col in training_columns:
    if col not in aligned_df.columns:
        aligned_df[col] = 0

# ===============================
# 4. Reorder to match training exactly
# ===============================
aligned_df = aligned_df[training_columns]

print("Aligned live_df shape:", aligned_df.shape)
print("Ready for scaler/model input")

# Get feature names from training data
expected_features = live_df.columns.tolist()

# Align new data with training features
X_new_aligned = X_new.reindex(columns=expected_features, fill_value=0)

# Scale and predict
X_new_scaled = scaler.transform(X_new_aligned)
y_pred = model.predict(X_new_scaled)

print("Predictions:", y_pred)

# Define mapping (short code → full description)
label_map = {
    "w": "Web traffic (text, browsing, http/https)",
    "d": "DNS (name lookups, text)",
    "u": "UDP apps (could be games/streaming → often audio/video)",
    "p": "P2P (file sharing, torrents → could be video/audio but mostly 'p2p')",
    "l": "LDAP (directory service → text/authentication traffic)",
    "m": "Mail (SMTP/IMAP/POP3 → text/email)"
}

# Numeric prediction mapping (example: 0-5 → short code)
num_to_code = {
    0: "d",
    1: "m",
    2: "p",
    3: "u",
    4: "w",
    5: "l"
}

# Predict
y_pred = model.predict(X_new_scaled)

# Convert numeric → code → full description
y_pred_labels = [label_map[num_to_code[num]] for num in y_pred]

# ✅ Print legend for user understanding
print("Legend:")
for code, meaning in label_map.items():
    print(f"{code} → {meaning}")

print("\nPredictions:")
for i, pred in enumerate(y_pred_labels, 1):
    print(f"{i} → {pred}")

